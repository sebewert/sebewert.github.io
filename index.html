<!DOCTYPE HTML>
<html>
	<head>
		<title>Sebastian Ewert</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<link href='https://fonts.googleapis.com/css?family=Raleway:400,100,200,300,500,600,700,800,900' rel='stylesheet' type='text/css'>
		<!--[if lte IE 8]><script src="js/html5shiv.js"></script><![endif]-->
		<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
		<script src="js/skel.min.js"></script>
		<script src="js/skel-panels.min.js"></script>
		<script src="js/init.js"></script>
		<noscript>
			<link rel="stylesheet" href="css/skel-noscript.css" />
			<link rel="stylesheet" href="css/style.css" />
			<link rel="stylesheet" href="css/style-desktop.css" />
		</noscript>
		<!--[if lte IE 8]><link rel="stylesheet" href="css/ie/v8.css" /><![endif]-->
		<!--[if lte IE 9]><link rel="stylesheet" href="css/ie/v9.css" /><![endif]-->

		<script type="text/javascript">
		<!--
		    function toggle_visibility(id) {
		       var e = document.getElementById(id);
		       if(e.style.display == 'block')
		          e.style.display = 'none';
		       else
		          e.style.display = 'block';
		    }

		    function showPhdSummary(id) {
				document.getElementById(id).style.display = 'block';
		    }
		    function hidePhdSummary(id) {
				document.getElementById(id).style.display = 'none';
		    }
		//-->
		</script>
	</head>
	<body>

		<!-- Header -->
		<div id="header">
			<div class="container">

				<!-- Logo -->
				<div id="logo">
					<h1><a href="#"></a></h1>
				</div>

				<!-- Nav -->
				<nav id="nav">
					<ul>
						<li class="active"><a href="index.html">Home</a></li>
						<li><a href="publications.html">Publications</a></li>
					</ul>
				</nav>

			</div>
		</div>
		<!-- Header -->

		<!-- Banner -->
		<div id="banner">
			<div class="container">

			</div>
		</div>
		<!-- /Banner -->

		<!-- Main -->
		<div id="main">
			<!-- Welcome -->
			<div id="welcome" class="container">
				<div class="row">
					<div id="content" class="9u skel-cell-important">
						<section>
							<header>
								<h1>Sebastian Ewert</h1>
							</header>

							<p><em><strong>Note:</strong> I have moved to Spotify. This website is effectively a remainder of my time in academia but I still update the list of publications.</em></p>

						 	<p>I am interested in <em>Machine Listening</em> and <em>Semantic Music Processing</em>, i.e. developing novel methods combining <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a>, <a href="https://en.wikipedia.org/wiki/Signal_processing">signal processing</a>, <a href="https://en.wikipedia.org/wiki/Statistical_model">statistical modelling</a>, <a href="https://en.wikipedia.org/wiki/Mathematical_optimization">numerical optimization</a> and many other fields. Such methods can then be applied to audio, music and multimedia data to identify hidden, semantically meaningful structure in raw data - this enables the development of intelligent, efficient and intuitive ways to search, re-use, explore or process audio in new ways. This includes, for example, methods for:
							<ul style="list-style-type: circle; margin-left: 3em;">
							<li><a href="publications.html#StollerED2018_AdversarialSourceSep_ICASSP">extracting individual instrument tracks</a> from a given music recording in high quality for remixing and upmixing purposes,</li>
							<li>detecting <a href="publications.html#EwertS16_PianoTranscriptionADMM_TASLP">which notes are being played</a> in a recording,</li>
							<li><a href="publications.html#MuellerCKEF10_Sync_ISR">finding and browsing music</a> in huge databases in efficient and intuitive ways,</li>
							<li>automatically giving music students <a href="publications.html#EwertWMS16_ScoreDeviation_ISMIR">feedback on their performance</a>,</li>
							<li>rendering the <a href="publications.html#WangED16_JointAlignment_TASLP">current position within a musical score</a> while listening to a piece of music,</li>
							<li>and many more applications, see also <a href="publications.html">publications</a>.</li>
							</ul>
							My background is in Computer Science and Mathematics. I did a PhD in Computer Science at the University of Bonn, Germany, which was supervised at the Max-Planck-Institute for Informatics, Saarbr&uuml;cken, Germany. Currently, I am a senior research scientist at Spotify. Before joining Spotify, I was a lecturer (â‰ˆassistant professor) for Signal Processing in the School of Electronic Engineering and Computer Science at Queen Mary University of London, conducting research within the Centre For Digital Music (<a href="http://c4dm.eecs.qmul.ac.uk">C4DM</a>, <a href="https://www.youtube.com/watch?v=Rcbs4NvMFHM">Intro Video</a>), where I was one of the founders of the <a href="http://machine-listening.eecs.qmul.ac.uk/">Machine Listening Lab</a>.</p>
						</section>

						<section>
							<header>
								<h2>Current and Past PhD Students</h2>
							</header>

							<p><span style="font-weight: bold;">Daniel Stoller:</span> <i>Machine Listening with Limited Annotations</i><br />
							<span class="phdsummarybrief" id="phdsummarybrief_daniel"><b>Summary:</b> How can we build machine listening systems that learn concepts based on examples with few, weak, or differently structured labels? Recent deep learning methods typically require large, precisely annotated datasets to generalise well. Models for smaller datasets are often regularised by capacity constraints or problem-specific assumptions, both limiting the potential performance. However, humans learn complex concepts based on only few explicitly labelled examples, arguably through structures acquired during extensive unsupervised exposure to the environment. Therefore, we investigate semi-supervised generative models, which allow us to flexibly incorporate prior assumptions about the data generation process to help generalisation...
							  <span class="spanLink" onclick="showPhdSummary('phdsummaryfull_daniel');hidePhdSummary('phdsummarybrief_daniel');">Show full</span>
						    </span>
							<span class="phdsummaryfull" id="phdsummaryfull_daniel"><b>Summary:</b> How can we build machine listening systems that learn concepts based on examples with few, weak, or differently structured labels? Recent deep learning methods typically require large, precisely annotated datasets to generalise well. Models for smaller datasets are often regularised by capacity constraints or problem-specific assumptions, both limiting the potential performance. However, humans learn complex concepts based on only few explicitly labelled examples, arguably through structures acquired during extensive unsupervised exposure to the environment. Therefore, we investigate semi-supervised generative models, which allow us to flexibly incorporate prior assumptions about the data generation process to help generalisation. Classification can then be expressed as inferring the latent structure given the data. To leverage more of the available data, we also apply multi-task learning to integrate information from related annotations. We will demonstrate these techniques in the field of music information retrieval: A combined singing voice separation and detection model will be developed to exploit the dependencies between these tasks and that benefits from prior information at test time. We deal with the problem of weak labels in the context of a lyrics alignment system, integrating annotations at the phoneme, word, and phrase level. Finally, we combine the above systems for a unified model of the singing voice that performs detection, separation, and transcription flexibly depending on the available data.
								<span class="spanLink" onclick="hidePhdSummary('phdsummaryfull_daniel');showPhdSummary('phdsummarybrief_daniel');">Hide full</span>
							</span>
							</p>


							<p><span style="font-weight: bold;">Delia Fano Yela:</span> <i>Signal Processing and Machine Learning Methods for Source Separation in Music Production</i><br />
							<span class="phdsummarybrief" id="phdsummarybrief_delia"><b>Summary:</b> In recent years, source separation has been a central research topic in music signal processing, with applications in stereo-to-surround up-mixing, remixing tools for DJs or producers, instrument-wise equalizing, karaoke systems, and pre-processing in music analysis tasks. This PhD focuses on various applications of source separation technique in the music production process, from removing interfering sound sources from studio and live recordings to tools for modifying the singing voice. In this context, most previous methods often specialize on so called stationary and semi-stationary interferences, such as simple broadband noise, feedback or reverberation. In practise, however, one often faces a variety of complex, non-stationary interferences, such as coughs, door slams or traffic noise...
							  <span class="spanLink" onclick="showPhdSummary('phdsummaryfull_delia');hidePhdSummary('phdsummarybrief_delia');">Show full</span>
						    </span>
							<span class="phdsummaryfull" id="phdsummaryfull_delia"><b>Summary:</b> In recent years, source separation has been a central research topic in music signal processing, with applications in stereo-to-surround up-mixing, remixing tools for DJs or producers, instrument-wise equalizing, karaoke systems, and pre-processing in music analysis tasks. This PhD focuses on various applications of source separation technique in the music production process, from removing interfering sound sources from studio and live recordings to tools for modifying the singing voice. In this context, most previous methods often specialize on so called stationary and semi-stationary interferences, such as simple broadband noise, feedback or reverberation. In practise, however, one often faces a variety of complex, non-stationary interferences, such as coughs, door slams or traffic noise. General purpose methods applicable in this context often employ techniques based on non-negative matrix factorization. Such methods use a dictionary of spectral templates that is computed using available training data for each interference class. A major problem here is that the training material often differs substantially in terms of spectral and temporal properties from the noise found in a given recordings, and thus such methods often fail to properly model the sound source and therefore fail to produce separation results of high or even acceptable quality. A major goal of this PhD will be to explore and develop conceptually novel source separation methods that go beyond dictionary-based state-of-the-art methods and yield results of high quality even in difficult scenarios.
								<span class="spanLink" onclick="hidePhdSummary('phdsummaryfull_delia');showPhdSummary('phdsummarybrief_delia');">Hide full</span>
							</span>
							</p>

							<p><span style="font-weight: bold;">Siying Wang:</span> <i>Computational Methods for the Alignment and Score-Informed Transcription of Piano Music</i><br />
							<span class="phdsummarybrief" id="phdsummarybrief_siying"><b>Summary:</b> The goal of music alignment is to establish links between different versions of a piece of music by mapping each position in one version to a corresponding position in another. Although alignment methods have considerably improved in accuracy in recent years, the task remains challenging. In particular, musicians interpret a musical score in a variety of ways leading to complex differences on a musical level between individual performances. Additionally, the wide range of possible acoustic conditions adds another layer of complexity to the task. Thus, even state-of-the-art methods fail in identifying a correct alignment if such differences are substantial. A first goal of this PhD...
							  <span class="spanLink" onclick="showPhdSummary('phdsummaryfull_siying');hidePhdSummary('phdsummarybrief_siying');">Show full</span>
						    </span>
							<span class="phdsummaryfull" id="phdsummaryfull_siying"><b>Summary:</b> The goal of music alignment is to establish links between different versions of a piece of music by mapping each position in one version to a corresponding position in another. Although alignment methods have considerably improved in accuracy in recent years, the task remains challenging. In particular, musicians interpret a musical score in a variety of ways leading to complex differences on a musical level between individual performances. Additionally, the wide range of possible acoustic conditions adds another layer of complexity to the task. Thus, even state-of-the-art methods fail in identifying a correct alignment if such differences are substantial. A first goal of this PhD is to increase the robustness for these cases by developing novel sequence models and alignment methods that can make use of specific information available in music synchronization scenarios. A first strategy is to exploit that in many scenarios not only two but multiple versions need to be aligned. By processing these jointly, we can supply the alignment process with additional examples of how a section might be interpreted or which acoustic conditions may arise. This way, we can use alignment information between two versions transitively to stabilize the alignment with a third version. Another general strategy is to rethink assumptions made in previous methods and how these might affect the alignment result. In particular, to increase the overall robustness, current methods typically assume that notes occurring simultaneously in the score are played concurrently in a performance. Musical voices such as the melody, however, are often played asynchronously to other voices, which can lead to significant local alignment errors. Therefore, this PhD develops novel methods that handle asynchronies between the melody and the accompaniment by treating the voices as separate timelines in a multi-dimensional variant of dynamic time warping (DTW). Constraining the alignment with information obtained via classical DTW, these methods measurably improve the alignment accuracy for pieces with asynchronous voices and preserves the accuracy otherwise. <br />
								Once an accurate alignment between a score and an audio recording is available, we can exploit the score information as prior knowledge in automatic music transcription (AMT), for scenarios such as music tutoring where score is available. We use score-informed dictionary learning technique to learn for each pitch spectral patterns describing the energy distribution of the associated notes in the recording. More precisely, we constrain the dictionary learning process in non-negative matrix factorization (NMF) using the aligned score. This way, by adapting the dictionary to a given recording, we achieve an improved accuracy compared to the state of the art.
								<span class="spanLink" onclick="hidePhdSummary('phdsummaryfull_siying');showPhdSummary('phdsummarybrief_siying');">Hide full</span>
								</span>

							</p>

							<!--<p>If you are interested in doing a PhD, please contact me (informally).</p>-->
						</section>
					</div>
					<div id="sidebar" class="3u">
						<section>
							<header>
								<h2></h2>
							</header>
							<a class="twitter-timeline" data-height="700" data-dnt="true" data-partner="tweetdeck" href="https://twitter.com/sebastian_ewert/timelines/887656757857746945">Tweets by @sebastian_ewert</a>
							<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
							<p style="font-size: 0.7em;">Design: <a href="http://templated.co">templated.co</a></p>
						</section>
					</div>
				</div>
			</div>
			<!-- /Welcome -->

		</div>
		<!-- /Main -->

	</body>
</html>
